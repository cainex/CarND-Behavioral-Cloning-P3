# **Behavioral Cloning** 

**Behavioral Cloning Project**

The goals / steps of this project are the following:

* Use the simulator to collect data of good driving behavior
* Build, a convolution neural network in Keras that predicts steering angles from images
* Train and validate the model with a training and validation set
* Test that the model successfully drives around track one without leaving the road
* Summarize the results with a written report


[//]: # (Image References)

[image1]: ./images/nvidia-sdc-cnn.png "Model Visualization"
[image2]: ./images/images.png "Training Images"
[image3]: ./images/cropped_images.png "Cropped Training Images"

## Rubric Points

### Here I will consider the [rubric points](https://review.udacity.com/#!/rubrics/432/view) individually and describe how I addressed each point in my implementation.  

---
### Files Submitted & Code Quality

#### 1. Submission includes all required files and can be used to run the simulator in autonomous mode

My project includes the following files:
* model.py containing the script to create and train the model
* drive.py for driving the car in autonomous mode
* model.h5 containing a trained convolution neural network 
* writeup.md summarizing the results
* test_run.mp4 is a video file of the autonomous run around the track using model.h5

#### 2. Submission includes functional code
Using the Udacity provided simulator and my drive.py file, the car can be driven autonomously around the track by executing 
```sh
python drive.py model.h5
```

Be sure to use the TensorFlow Backend. This may require:
```sh
KERAS_BACKEND=tensorflow python drive.py model.h5
```

#### 3. Submission code is usable and readable

The model.py file contains the code for training and saving the convolution neural network. The file shows the pipeline I used for training and validating the model, and it contains comments to explain how the code works.

### Model Architecture and Training Strategy

#### 1. An appropriate model architecture has been employed

I used the CNN configuration that NVIDIA used in the paper [End to End Learning for Self-Driving Cars](https://arxiv.org/abs/1604.07316) and further explained in NVIDIA's devblog article [End-to-End Deep Learning for Self-Driving Cars](https://devblogs.nvidia.com/parallelforall/deep-learning-self-driving-cars/)

This is a 9 layer convolutional neural network consisting of 5 convolution layers (2 5x5 layers followed by 3 3x3 layers) and 4 fully-connected layers.

![alt text][image1]

The network is built using Keras in the build_model function inside model.py (line 76). 

Addtional reference came from Siraj Rival's [How to Simulate a Self-Driving Car](https://www.youtube.com/watch?v=EaY5QiZwSP4&feature=youtu.be) youtube video.

#### 2. Attempts to reduce overfitting in the model

The model contains a dropout layer after the convolution layers with 50% dropout in order to reduce overfitting (model.py line 88). 

The model was trained and validated on different data sets to ensure that the model was not overfitting. The model was tested by running it through the simulator and ensuring that the vehicle could stay on the track.

#### 3. Model parameter tuning

The model used an adam optimizer, so the learning rate was not tuned manually (model.py line 122).

#### 4. Appropriate training data

Training data was generated by completing four complete laps on track 1 and an additional 2 complete laps on the more challenging track 2.

For details about how I created the training data, see the next section. 

### Model Architecture and Training Strategy

#### 1. Solution Design Approach

I began with the NVIDIA CNN, so the overall strategy centered around collecting the necessary amount of training data, and adequately pre-processing that data.

My first step was to implement the CNN in Keras based on the NVIDIA model with the sample dataset provided. Initially only the center image was used with only image normalization lambda layer (which divides by 127.5 and subtract 0.5 from each pixel). The data set was initially not trained using a generator. This produced poor results, however, I was able to get the network created, and the initial training and inference runs working.

As a side note, I trained the model on my local workstation using a GTX 1080 GPU with GPU accelerate TensorFlow as the Keras backend. However, I ran into issues using the GPU acceleration for inference, hitting a cudnn device allocation error. I believe this has to do with TensorFlow allocating nearly all of the GPU memory by default. This was only an issue when the simulator was also running.

It's possible this could have been worked around by setting the GPU to grow memory in the drive.py script:
```sh
config = tf.ConfigProto()
config.gpu_options.allow_growth = True
session = tf.Session(config=config, ...)
```

However, I worked around this by performing inference using the CPU and not the GPU accelerate TensorFlow backend. Inference on the CPU worked well.

At this point, I collected my own initial dataset by driving around the simulated track for 4 complete laps. With the larger dataset, I then created the python generator to return batches to the model during training. I had the generator return the center, left and right images. The dataset was also split into a training set (80%) and a validation set (20%) using sklearn test_train_split. I also add a model checkpoint callback for Keras to save only the best training run along with early stopping (however, with 20 epochs, there early stopping didn't kick in).

With this, the results were better, but the model still had some issues getting around the track completely. I then proceded to do 3 things. First, I created a second dataset by driving 2 complete laps around track 2. Second, I added data augmentation by adding flipped versions of the center, left and right images in the generator. Finally, I added a cropping layer to the model to crop the top and bottom of the image.

With these changes, the model was able to drive the car completely around track 1, and nearly able to drive the car around track 2. On track 2, the model had one spot where it had difficulty were the track converges with an earlier part of the track. I believe this could be fixed in the model by collecting more training data from that section of track.

#### 2. Final Model Architecture

The final network model was based on the CNN used by NVIDIA. The dropout layer was added following the convolution layers, and a normization lambda layer along with a cropping layer were added to the front of the network. The Keras summary is:

```sh
Layer (type)                 Output Shape              Param #   
=================================================================
lambda_1 (Lambda)            (None, 160, 320, 3)       0         
_________________________________________________________________
cropping2d_1 (Cropping2D)    (None, 90, 320, 3)        0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 43, 158, 24)       1824      
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 20, 77, 36)        21636     
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 8, 37, 48)         43248     
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 6, 35, 64)         27712     
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 4, 33, 64)         36928     
_________________________________________________________________
dropout_1 (Dropout)          (None, 4, 33, 64)         0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 8448)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 100)               844900    
_________________________________________________________________
dense_2 (Dense)              (None, 50)                5050      
_________________________________________________________________
dense_3 (Dense)              (None, 10)                510       
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 11        
=================================================================
Total params: 981,819
Trainable params: 981,819
Non-trainable params: 0

```
This model is created in the build_model function in model.py (line 76)

#### 3. Creation of the Training Set & Training Process

The training data was captured by completing 4 complete laps around track 1 in the first dataset, and 2 complete laps around track 2 in the second dataset. The CSV files were then concatenated together, and the two datasets were trained together. 

| | frames | images | 
|---|---|---|
| **total** | 15838 | 95028 |
| **training** | 12672 | 76032 |
| **validation** | 3168 | 19008 |

The trained data consisted of the three captured images (center left and right) along with a flipped version of each of those images. For each frame, the center, left and right images are provided, but only a single driving angle. The steering angle needed to be modified for all resulting images except the original center image. The steering angle was modified:

| image | steering angle |
|:-----:|:--------------:|
| center | steering_angle |
| left | steering_angle + 0.2 |
| right | steering_angle - 0.2 |
| center flipped | -steering_angle |
| left flipped | -(steering_angle + 0.2) |
| right flipped | -(steering_angle - 0.2) |

An example is shown below:
![alt text][image2]

The data augmetation was done in the batch generator. The pre-processing steps (image cropping and normalization) were performed by the model itself. This was to allow the normalization and cropping to be easily performed both during training and during inference (pre-processing was also performed on the images recieved from the simulator during the autonomous run). The image cropping can be seen below:

![alt text][image3]

The batch generator also shuffled the data set at the beginning and after each batch was generated.

The dataset was split into a training set (80%) and a validation set (20%). A generator was created for the training set and a second generator was created for the validation set:

```sh
    train_generator = generator(train_samples, batch_size=args.batch_size)
    validation_generator = generator(validation_samples)
```

The batch size passed into the generator refers to how many frames to pull for each batch. The number of images returned for each batch is 6*batch_size. The training was done with a batch_size=32.

The batch generators were fed into the training of the model along with two callbacks. The first callback will save off the best fit model, the second callback will stop the trianing early if the validation error begins to rise.

```sh
model.fit_generator(train_generator, steps_per_epoch=num_steps_per_epoch, validation_data=validation_generator, validation_steps=num_validation_steps, epochs=args.num_epochs, callbacks=[checkpoint, early_stop], verbose=1)
```

The model was trianed for 20 epochs using the adam optimizer with a learning rate of 0.001.
